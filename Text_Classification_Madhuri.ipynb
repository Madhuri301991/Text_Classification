{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb02de30",
   "metadata": {},
   "source": [
    "# Text Classification Project\n",
    "Now we're at the point where we should be able to:\n",
    "* Read in a collection of documents - a *corpus*\n",
    "* Transform text into numerical vector data using a pipeline\n",
    "* Create a classifier\n",
    "* Fit/train the classifier\n",
    "* Test the classifier on new data\n",
    "* Evaluate performance\n",
    "\n",
    "For this project we'll use the Cornell University Movie Review polarity dataset v2.0 obtained from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c5f62",
   "metadata": {},
   "source": [
    "## Perform imports and load the dataset\n",
    "The dataset contains the text of 2000 movie reviews. 1000 are positive, 1000 are negative, and the text has been preprocessed as a tab-delimited file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3486f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c720fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('moviereviews.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "#have 2000 reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b15bfe",
   "metadata": {},
   "source": [
    "### Take a look at a typical review. This one is labeled \"negative\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the raw text, predict the movie is positive or negative\n",
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8caa45",
   "metadata": {},
   "source": [
    "## Check for missing values:\n",
    "\n",
    "### Detect & remove NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb17781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the existence of NaN values in a cell:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f855b",
   "metadata": {},
   "source": [
    "35 records show **NaN** (this stands for \"not a number\" and is equivalent to *None*). These are easily removed using the `.dropna()` pandas function.\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">CAUTION: By setting inplace=True, we permanently affect the DataFrame currently in memory, and this can't be undone. However, it does *not* affect the original source data. If we needed to, we could always load the original DataFrame from scratch.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ee8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21719d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93bdb5",
   "metadata": {},
   "source": [
    "### Detect & remove empty strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blanks=[]    # start with an empty list\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "            \n",
    "print(len(blanks),'blanks',blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14db0f",
   "metadata": {},
   "source": [
    "Next we'll pass our list of index numbers to the **.drop()** method, and set `inplace=True` to make the change permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blanks list contain all the ids whose reviews are empty, drop them from the dataframe\n",
    "df.drop(blanks,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff790cfe",
   "metadata": {},
   "source": [
    "Great! We dropped 62 records from the original 2000. Let's continue with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28845a",
   "metadata": {},
   "source": [
    "## Take a quick look at the `label` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cadce",
   "metadata": {},
   "source": [
    "## Split the data into train & test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X=df['review']\n",
    "y=df['label']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9be27",
   "metadata": {},
   "source": [
    "## Build pipelines to vectorize the data, then train and fit a model\n",
    "Now that we have sets to train and test, we'll develop a selection of pipelines, each with a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd93b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#naive_bayes\n",
    "text_clf_nb=Pipeline([('tfidf',TfidfVectorizer()),('clf',MultinomialNB())])\n",
    "\n",
    "#Linear SVC\n",
    "text_clf_lsvc=Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51d0b3",
   "metadata": {},
   "source": [
    "## Feed the training data through the first pipeline\n",
    "We'll run naïve Bayes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e01ff",
   "metadata": {},
   "source": [
    "## Run predictions and analyze the results (naïve Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b363bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predications=text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2106b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ac705",
   "metadata": {},
   "source": [
    "Naïve Bayes gave us better-than-average results at 76.4% for classifying reviews as positive or negative based on text alone. Let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a7851",
   "metadata": {},
   "source": [
    "## Feed the training data through the second pipeline\n",
    "Next we'll run Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_lsvc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06765f51",
   "metadata": {},
   "source": [
    "## Run predictions and analyze the results (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predications=text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69248cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c196e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf826d",
   "metadata": {},
   "source": [
    "Not bad! Based on text alone we correctly classified reviews as positive or negative **84.7%** of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b542fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
